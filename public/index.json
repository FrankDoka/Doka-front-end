[{"content":"-Created basic diagram for the project -created outline of the project details -started working on implementing basic AWS cognito rudimentary login system -implemented with GUI and then converted implementation to Terraform\n","permalink":"https://frankdoka.com/blog/login-system-1/","summary":"-Created basic diagram for the project -created outline of the project details -started working on implementing basic AWS cognito rudimentary login system -implemented with GUI and then converted implementation to Terraform","title":"Login System Part 1"},{"content":"Setting Up Project Details Description : Create a user login system that will integrate AWS Services. Have a user profile page and use AWS Cognito to allow users to have certain functions depending on the authorization of the user. Offer services such as text to speech in various languages conversions and also services such as offering document conversion. Offer possible AI image generation service among other services.\nAWS Services Used AWS Serices Used: AWS Cognito, AWS Lambda, DynamoDB, SES, SNS, SQS, API Gateway, AWS Polly and possibly more.\nFrontend and Backend Frontend: Login page with Registration (requiring username / password / email / bday etc.) and button to register / login to the system. After Login: Default to user profile page with information and specific tabs that user has access too. Also have option to logout of the system.\nBackend: AWS Cognito to handle User pool and Authentication. AWS Lambda functions to do handle user authorization credentials. DynamoDB to store user data and other data. AWS S3 to store text files for conversion and documents for conversion. SES/SNS/SQS? To confirm email for validity of user creation and also for forgot password. API + API Gateway to take data input from user. AWS Polly text to speech conversion in multiple languages Session management - best practices?\nAdditional procedures Use CI/CD Methods + Github + Github actions to provide pipeline for the frontend and the backend components. Use Terraform for the creation and update / maintanence of the resources required for the project. Deploy on Dev environment + testing before moving to Production environment. Design tool - Excalidraw - https://excalidraw.com/.\nSteps Initial setup of the Project. Writing project description and details along with desired AWS services to use. Design Diagram of the Project. Implement rudimentary frontend + backend Login system using AWS Cognito. Implement advanced frontend user interface with user profile page and access rights to certain content depending on user level. With login system completed - start work on Text to speech conversion tool. ","permalink":"https://frankdoka.com/projects/frank-doka-project-2/","summary":"Setting Up Project Details Description : Create a user login system that will integrate AWS Services. Have a user profile page and use AWS Cognito to allow users to have certain functions depending on the authorization of the user. Offer services such as text to speech in various languages conversions and also services such as offering document conversion. Offer possible AI image generation service among other services.\nAWS Services Used AWS Serices Used: AWS Cognito, AWS Lambda, DynamoDB, SES, SNS, SQS, API Gateway, AWS Polly and possibly more.","title":"Project Details - Login System and Text to Speech Conversion"},{"content":"Smoke Testing, Infrastructure as Code (IAC), and CI/CD Enhancements In this phase of my project, I focused on enhancing my project with smoke testing, delved into Infrastructure as Code (IAC) using AWS Serverless Application Model (SAM) and Terraform, and continued to refine my Continuous Integration/Continuous Deployment (CI/CD) pipeline.\nSmoke Testing Install Cypress: Installing Cypress, a powerful end-to-end testing framework. Cypress would be instrumental in performing smoke tests to validate my system\u0026rsquo;s functionality.\nSmoke Test Execution: With Cypress in place, I conducted smoke tests to check the data being sent and to cover edge cases. These tests ensured that my system behaved as expected under various scenarios.\nInfrastructure as Code (IAC) AWS SAM Template: I explored AWS Serverless Application Model (SAM) templates, a convenient way to define my serverless infrastructure. This included configuring components like Lambda functions, DynamoDB, and API Gateway.\nCloudFormation and Terraform: I delved into two IAC tools, CloudFormation and Terraform, to manage my cloud resources. CloudFormation is AWS\u0026rsquo;s native tool for IAC, while Terraform is a versatile, multi-cloud tool widely adopted in the industry. Terraform uses a domain-specific configuration language called HCL.\nCI/CD Backend GitHub Actions Workflow: I leveraged GitHub Actions to automate my CI/CD pipeline. This streamlined the process of deploying updates, ensuring that my SAM application package and Python code were properly tested before deployment.\nWorkflow Steps:\nBuild Step: In this step, I packaged up my Lambda function code and installed any necessary dependencies. The output was a zipped Lambda function artifact and a configuration file ready for deploying resources. Deploy Step: I utilized my chosen IAC tool (Terraform) to deploy AWS resources based on the defined templates. Smoke Test Step: To ensure the API functioned as expected, I ran Cypress API tests, confirming that the deployed API met my requirements. Backend Infrastructure with Terraform Terraform Configuration: I began setting up my backend infrastructure as code using Terraform. This included creating a main.tf file and configuring an EC2 instance for testing purposes.\nLambda Function and DynamoDB Policies: I reviewed my Lambda function code and ensured that the necessary policies and permissions were correctly defined.\nDynamoDB Table Configuration: I created the base DynamoDB table and continued to explore how to add items to it programmatically.\nAPI Gateway Enhancements: I improved my API Gateway configuration by attaching the Lambda function and configuring the GET function. Additionally, I addressed CORS (Cross-Origin Resource Sharing) settings, allowing the GET method.\nGitHub Repository Setup Terraform and Backend Repository: I initiated a GitHub repository dedicated to my backend code and testing. This repository would host my Terraform configurations and serve as a central hub for backend development. I\u0026rsquo;ve made significant strides in my project, focusing on testing, infrastructure as code, and CI/CD enhancements. My commitment to automation and robust testing ensures a reliable and scalable project.\n","permalink":"https://frankdoka.com/blog/resume-challenge-5/","summary":"Smoke Testing, Infrastructure as Code (IAC), and CI/CD Enhancements In this phase of my project, I focused on enhancing my project with smoke testing, delved into Infrastructure as Code (IAC) using AWS Serverless Application Model (SAM) and Terraform, and continued to refine my Continuous Integration/Continuous Deployment (CI/CD) pipeline.\nSmoke Testing Install Cypress: Installing Cypress, a powerful end-to-end testing framework. Cypress would be instrumental in performing smoke tests to validate my system\u0026rsquo;s functionality.","title":"IaC, CI/CD Enhancements, Smoke testing"},{"content":"Progress in API Backend and Front-End Integration In my journey into the world of API backend development, I initiated the crucial integration between my front-end and back-end systems.\nAPI Creation and Testing API Gateway Route for Lambda Function: I established an API Gateway route in front of my Lambda function, configuring it to handle POST requests. This setup paved the way for communication between my front-end and back-end components.\nInternal Testing with API Gateway: To ensure the functionality of my API, I harnessed the API Gateway\u0026rsquo;s internal test console. I rigorously tested the API by making requests to my Lambda function and confirming that the responses aligned with my expectations.\nExternal Testing with Postman: Expanding my testing scope, I employed Postman to test my API externally. This thorough testing approach verified that my API functioned seamlessly when accessed from external sources.\nIncrementing DynamoDB Count: One of my pivotal achievements involved creating an API Gateway for Lambda, enabling it to invoke the Lambda function to increment the count in my DynamoDB table. I conducted comprehensive testing through different channels, including browsers and Postman, to ensure the count incremented as intended.\nFront-End and Back-End Integration Connecting Front-End to Back-End: The next critical milestone was to establish a seamless connection between my front-end (hosted on S3) and the back-end (my API). My objective was to display the visitor count on the homepage of my website.\nAutomated Tests: I implemented automated testing procedures to validate the smooth operation of my integration. These tests ensured that my system consistently performed as expected.\nJavaScript Visitor Counter: I developed a JavaScript visitor counter that made HTTP requests to my API Gateway. Although this task was relatively straightforward, I encountered CORS (Cross-Origin Resource Sharing) issues between the browser and API. These issues were successfully addressed to enable the display of visitor counts on my web app.\nPython Code Testing: To ensure comprehensive testing, I created automated tests for my API. These tests, often referred to as \u0026ldquo;smoke tests\u0026rdquo; or end-to-end tests, verified the correctness of my Lambda code, the accuracy of permissions and configurations, and the successful deployment of resources. Cypress, a JavaScript testing framework, was utilized to call my API endpoint and perform a series of checks, including verifying updated values, correct database updates, handling of unexpected input, and edge case scenarios.\nFront-End Updates and CI/CD: I revisited the front end, integrating it into my GitHub repository and enhancing my CI/CD (Continuous Integration/Continuous Deployment) pipeline. I also reinforced security by updating AWS secret keys for access.\nHugo Shortcode for Visitor Count: To simplify the display of the visitor count on my website, I crafted a Hugo shortcode and introduced a new Visitors tab for shortcode testing.\nFront-End Integration Challenges: While the front end successfully fetched data from DynamoDB and displayed the visitor count, challenges emerged when updating the count accurately. These challenges primarily stemmed from CORS policy restrictions.\nCORS Policy Adjustment: To overcome the CORS issue, I made essential changes to the CORS policy on AWS. These adjustments allowed for seamless communication between the website and the API, including GET method access.\nWith these achievements and challenges addressed, I am making substantial progress toward achieving a seamless integration between my front-end and back-end systems.\n","permalink":"https://frankdoka.com/blog/resume-challenge-4/","summary":"Progress in API Backend and Front-End Integration In my journey into the world of API backend development, I initiated the crucial integration between my front-end and back-end systems.\nAPI Creation and Testing API Gateway Route for Lambda Function: I established an API Gateway route in front of my Lambda function, configuring it to handle POST requests. This setup paved the way for communication between my front-end and back-end components.\nInternal Testing with API Gateway: To ensure the functionality of my API, I harnessed the API Gateway\u0026rsquo;s internal test console.","title":"API Backend continuation and Frontend integration"},{"content":"Embarking on Backend Technologies and API Development I delved into the realm of backend technologies and APIs, laying the essential groundwork for the functionality of my project.\nJavaScript Code to API to Database (DynamoDB) My primary mission was to establish a seamless connection between my JavaScript code and the database. I chose Amazon DynamoDB as my database solution and employed AWS services like API Gateway and Lambda to construct a robust backend.\nSetting Up the Lambda Function My initial task involved configuring a Python Lambda function through the AWS Web Console. This setup paved the way for processing requests from my front-end application.\nDynamoDB Table Setup Creating a DynamoDB table was a straightforward process, achieved through simple \u0026ldquo;point and click\u0026rdquo; actions. This table serves as my data store, facilitating efficient data storage and retrieval. I then took this a step further and created a local DynamoDB database and learned to create the database using IaC.\nWriting Python Code for Lambda To empower my Lambda function, I meticulously crafted Python code to interact with DynamoDB. Rigorous testing from the AWS Console ensured successful data storage in the database.\nIAM Permissions Configuration Safeguarding access between my Lambda function and the DynamoDB table remained a priority. I meticulously configured IAM permissions to manage access and maintain control over interactions with these AWS resources.\nExploring SAM (Serverless Application Model) My journey included an exploration of SAM, an open-source framework tailored for building serverless applications. My aim was to set up SAM locally, enabling the development and testing of Lambda functions and DynamoDB tables in a local environment before deployment to AWS.\nCreating the DynamoDB Database in AWS Within my AWS environment, I established a DynamoDB database and seamlessly configured the Lambda function\u0026rsquo;s connection to it. IAM permissions were thoughtfully set up to facilitate smooth communication.\nAPI Development With my backend infrastructure in place, I embarked on the development of the API. This API will serve as the conduit for communication between my front-end website and the Lambda function, facilitating seamless data exchange.\nAPI Gateway Configuration My choice for managing API endpoints was API Gateway. Skillful configuration ensured that requests were efficiently routed to the Lambda function, enabling effective data transfer between the website and the backend.\nTesting the API Locally I leveraged the internal test console of the API Gateway to conduct a \u0026ldquo;Hello World\u0026rdquo; test, ensuring the correctness of the API\u0026rsquo;s connection to the Lambda function.\nExternal Testing with Postman To confirm the readiness of my API for real-world usage, I obtained its public URL from the API Gateway and subjected it to rigorous testing using external API testing tools such as Postman. My objective was to verify that the Lambda function produced the expected output.\nWith these strategic steps, I\u0026rsquo;ve made significant progress in establishing my backend infrastructure and API.\n","permalink":"https://frankdoka.com/blog/resume-challenge-3/","summary":"Embarking on Backend Technologies and API Development I delved into the realm of backend technologies and APIs, laying the essential groundwork for the functionality of my project.\nJavaScript Code to API to Database (DynamoDB) My primary mission was to establish a seamless connection between my JavaScript code and the database. I chose Amazon DynamoDB as my database solution and employed AWS services like API Gateway and Lambda to construct a robust backend.","title":"Backend technologies and APIs"},{"content":"Front-End Development in the Cloud Resume Project In this phase of my Cloud Resume Project, I ventured into building the front-end of my project, mastering various tools and techniques to create a polished web presence.\nCrafting a Resume in HTML My journey began with creating a digital resume. I opted for the simplicity and precision of HTML to structure my content exactly as desired, laying the foundation for future styling.\nStyling with CSS To elevate my resume\u0026rsquo;s visual appeal, I harnessed the power of Cascading Style Sheets (CSS). CSS enabled me to transform plain HTML content into an aesthetically pleasing document, effectively showcasing my skills and experience.\nHosting with S3, HTTPS, and DNS With my resume ready, the next step was making it globally accessible. I hosted it as a static website on Amazon S3, ensuring secure delivery over HTTPS using Amazon CloudFront and configuring DNS for seamless access.\nConfiguring Route 53 and Routing Policies To guarantee uninterrupted website access, I leveraged Amazon Route 53. This involved configuring routing policies and creating essential DNS records.\nAddressing Name Server Issues Managing name servers posed a challenge. I configured my domain with the correct name servers of the hosted zone to ensure precise domain resolution.\nEnhanced Hosting with CloudFront For heightened website performance and security, I implemented Amazon CloudFront. This content delivery network (CDN) acted as an intermediary, facilitating faster load times and enhanced security.\nSSL Certificate from ACM Security was paramount, so I fortified my website with an SSL certificate courtesy of AWS Certificate Manager (ACM). This crucial step provided encryption and instilled trustworthiness, enhancing visitor safety.\nDesigning with Hugo To infuse a professional and polished aesthetic into my website, I turned to Hugo. This static site generator empowered me to craft a visually appealing and responsive website with ease.\nGitHub and GitHub Actions for Deployment Efficient deployment was achieved through GitHub and GitHub Actions, streamlining the process and enabling automation. I did encounter image display challenges during this phase.\nOvercoming Image Display Challenges To address image display issues, I implemented solutions like PaperMod and fine-tuned content and static paths on my Hugo HTML website. These adjustments ensured images were displayed correctly, enhancing the overall user experience.\n","permalink":"https://frankdoka.com/blog/resume-challenge-2/","summary":"Front-End Development in the Cloud Resume Project In this phase of my Cloud Resume Project, I ventured into building the front-end of my project, mastering various tools and techniques to create a polished web presence.\nCrafting a Resume in HTML My journey began with creating a digital resume. I opted for the simplicity and precision of HTML to structure my content exactly as desired, laying the foundation for future styling.","title":"Building the Frontend"},{"content":"Setting Up My Management Account Established a Management Account, which serves as the central hub for controlling my AWS resources. This account plays a pivotal role in orchestrating and managing my entire AWS infrastructure.\nBuilding an AWS Organization with org-formation tool To streamline the organization of my AWS accounts and resources, I\u0026rsquo;ve leveraged the powerful org-formation tool. This nifty tool automates the process, making it easier for me to configure and manage my AWS Organization. With org-formation, I can define the structure of my organization, establish clear hierarchies, and maintain consistency across my AWS environment.\nThis tool takes the idea behind AWS Control Tower and enhances it with more features and capabilities.\nEmbracing CodeCommit and CodePipeline To ensure efficient version control and continuous integration/continuous deployment (CI/CD) processes, I\u0026rsquo;ve embraced CodeCommit and CodePipeline.\nThese AWS services allow me to commit and push changes to my codebase seamlessly. With Git at my fingertips, I have a robust mechanism to manage code versions and automate the deployment pipeline.\nFor my frontend and backend solutions later on in the project, I\u0026rsquo;ll be utilizing GitHub and GitHub Actions.\nImplementing Automated Billing Alerts Set up automated billing alerts. These alerts notify me when my AWS costs reach predefined thresholds, ensuring that I am always aware of my spending.\nIntegrating CodeCommit with Visual Studio Code For a seamless development experience, I\u0026rsquo;ve integrated CodeCommit with Visual Studio Code (VS Code). This integration simplifies the code collaboration process and allows me to work efficiently within my preferred development environment.\nCreating a Development Account To maintain a clear separation between my production and development environments, I\u0026rsquo;ve established a dedicated Development Account. This account serves as a safe sandbox for testing changes before they are pushed to the production environment. It\u0026rsquo;s a crucial step to ensure the stability and reliability of my systems.\nSetting Up AWS Single Sign-On (SSO) Enhancing the login experience for AWS sign-on to multiple AWS accounts through the use of roles. I implemented AWS Single Sign-On (SSO) to provide a more convenient and secure way for to access AWS services. With SSO, I can access multiple AWS accounts and applications using a single set of credentials.\nLeveraging IAM Identity Center for Role Federation To further enhance my security posture and access management, I\u0026rsquo;ve utilized IAM Identity Center for role federation. This ensures that users have the appropriate permissions to access AWS resources while maintaining a high level of security and control.\nWith these foundational steps in place, I\u0026rsquo;ve laid the groundwork for a robust and organized AWS environment.\n","permalink":"https://frankdoka.com/blog/resume-challenge-1/","summary":"Setting Up My Management Account Established a Management Account, which serves as the central hub for controlling my AWS resources. This account plays a pivotal role in orchestrating and managing my entire AWS infrastructure.\nBuilding an AWS Organization with org-formation tool To streamline the organization of my AWS accounts and resources, I\u0026rsquo;ve leveraged the powerful org-formation tool. This nifty tool automates the process, making it easier for me to configure and manage my AWS Organization.","title":"AWS Accounts and Organization"},{"content":"Credentials ðŸ”— Credly Badge Introduction I passed SAA-C03 - AWS Solutions Architect - Associate certification exam.\nCertification Information As Cloud Computing continues to ascend, businesses are in a constant state of transition, moving away from traditional on-premise infrastructure towards the cloud. This migration brings a wealth of advantages, primarily in terms of scalability and resilience when confronted with unexpected disasters.\nThe AWS Solutions Architect - Associate certification stands as a testament to your proficiency in crafting and deploying meticulously planned solutions within the AWS ecosystem, currently the foremost cloud provider in the industry. In essence, this examination gauges your aptitude for conceiving architectural designs tailored to specific scenarios. To illustrate, consider a scenario where a company desires the uninterrupted operation of its application, even in the face of a catastrophic event causing an entire AWS region to become inoperable. In this context, how would you strategically structure their infrastructure to meet this exigent requirement?\nExam Format In this examination, you will encounter a total of 65 questions and have a time allotment of 130 minutes to respond to them. This equates to an average of 2 minutes per question for you to carefully consider and answer. Your evaluation will be established on a percentile scale that spans from 100 to 1000, with a requisite score of over 720 to attain a passing grade. With this information in mind, you can deduce that achieving approximately 72% accuracy in your responses is essential for success. For a more comprehensive understanding of the scoring system, you can consult this informative link.\nThis examination operates under a binary pass/fail criterion. Should you successfully pass the exam, the specific score achieved will have no substantial bearing. Your score will merely be included in your score report for personal reference, without appearing on the official certificate.\nThe examination fee for this assessment is set at $150 USD.\nPrep Strategy for SAA-C03 Exam Exam Blueprint Understanding: Begin by thoroughly reviewing the official AWS Certified Solutions Architect - Associate exam guide to understand key domains.\nStudy Plan: Create a flexible study schedule allocating time for courses, practice exams, hands-on labs, and revision.\nOnline Resources: Enroll in reputable online courses tailored for the SAA-C03 exam. Popular platforms like AWS Training and A Cloud Guru offer comprehensive video courses.\nHands-On Practice: Use a free AWS Free Tier account to gain practical experience deploying AWS services and solutions.\nPractice Exams: Take official AWS practice exams, available on the AWS Training website.\nCommunity Engagement: Join AWS forums, such as the AWS Developer Forums.\nRegular Review: Periodically revisit study materials to reinforce your understanding, focusing on weaker areas.\nExam Simulation: Simulate the exam environment with timed practice exams as your exam date approaches.\nIdentify Weaknesses: Analyze practice exam results to pinpoint areas needing improvement.\nExam Logistics: Familiarize yourself with exam details and arrive early on the exam day with proper identification.\nPost-Exam Analysis: Regardless of the outcome, review areas of difficulty for future improvement.\nContinuous Learning: Stay updated with AWS announcements through the AWS Blog, AWS What\u0026rsquo;s New, and consider advanced AWS certifications after achieving SAA-C03.\n","permalink":"https://frankdoka.com/blog/aws-saa-certification/","summary":"Credentials ðŸ”— Credly Badge Introduction I passed SAA-C03 - AWS Solutions Architect - Associate certification exam.\nCertification Information As Cloud Computing continues to ascend, businesses are in a constant state of transition, moving away from traditional on-premise infrastructure towards the cloud. This migration brings a wealth of advantages, primarily in terms of scalability and resilience when confronted with unexpected disasters.\nThe AWS Solutions Architect - Associate certification stands as a testament to your proficiency in crafting and deploying meticulously planned solutions within the AWS ecosystem, currently the foremost cloud provider in the industry.","title":"AWS SAA Certification Exam"},{"content":"My name is Frank Doka. I am currently located in New York City.\nThese days, Iâ€™m working with different cloud technologies and integrations such as AWS, containers, CI/CD and using infra-as-code tools such as Terraform.\nOne of my passions is Systems Engineering. Finding the best way to improve reliability and scalability of various technologies. Streamlining the process of pushing code into production.\nI have a keen interest in the field of Automation and applying it into the work I produce. Creating high quality and re-usuable code through scripts written in various languages is something that I strive to achieve.\nI have over 6 years of system engineering experience in the public sector working for the City of New York as a Systems Administrator. I am looking to advance my career and skillset into the cloud and incorporate my existing skills and knowledge to provide positive growth to the companies I work with as well as my own personal growth.\n","permalink":"https://frankdoka.com/about/","summary":"My name is Frank Doka. I am currently located in New York City.\nThese days, Iâ€™m working with different cloud technologies and integrations such as AWS, containers, CI/CD and using infra-as-code tools such as Terraform.\nOne of my passions is Systems Engineering. Finding the best way to improve reliability and scalability of various technologies. Streamlining the process of pushing code into production.\nI have a keen interest in the field of Automation and applying it into the work I produce.","title":"About Me"},{"content":"Intro The AWS Cloud Resume Challenge is an engaging, hands-on initiative designed to elevate AWS cloud skills. Participants are challenged to build a professional resume website hosted exclusively on AWS infrastructure. This practical project hones the ability to deploy, configure, and secure AWS services, including EC2, S3,Route 53, Lambda, DynamoDB and much more.\nIt offers an exceptional opportunity to showcase expertise to prospective employers while developing proficiency in cloud computing. Bolster AWS knowledge and create a tangible asset.\nProject Overview This is a high level overview of the steps I took during this challenge:\nHTML Resume: Start by creating a professional resume using HTML.\nCSS Styling: Style resume using CSS to enhance its visual appeal and formatting.\nStatic Website: Host resume on a static S3 website.\nHTTPS Security: Ensure the security of S3 website by enabling HTTPS. Utilize Amazon CloudFront for this purpose.\nCustom DNS: Point a custom DNS name to CloudFront distribution using Amazon Route 53 for a personalized web address.\nJavaScript Visitor Counter: Implement a visitor counter on website using JavaScript to display the number of site visitors.\nDatabase for Visitor Counter: Use DynamoDB to store and manage the visitor counter data efficiently, benefiting from its on-demand pricing model.\nAPI Creation: Develop an API that communicates with the database and accepts requests from web app. AWS API Gateway combined with Lambda can help accomplish this.\nPython Integration: Utilize Python for Lambda functions to enhance the functionality of serverless architecture.\nTesting: Implement robust testing for Python code to ensure its reliability and performance.\nInfrastructure as Code (IaC): Configure AWS resources like DynamoDB tables, API Gateway, and Lambda functions using an AWS SAM (Serverless Application Model) template. Deploy these resources effortlessly using AWS SAM CLI.\nSource Control: Employ GitHub for version control, enabling seamless updates to both the back-end API and front-end website.\nCI/CD (Backend): Implement GitHub Actions to automate tests and deployments. Whenever updates are pushed to SAM template or Python code, tests are executed, and if successful, the SAM app is packaged and deployed to AWS.\nCI/CD (Frontend): Maintain a separate GitHub repository for website code. Configure GitHub Actions to automatically update the S3 bucket when code changes occur (consider adding logic to invalidate CloudFront cache in code). Avoid committing AWS credentials to your code.\nBlog Post Integration: Include a link to a concise blog post within resume text. This post should highlight key learnings and experiences gained throughout the project.\nPhase 1: Creating AWS Accounts and Organization Blog Post: Phase 1\nCreating the initial AWS master account, production and development accounts. Using tools such as Org-Formations to automate the process for version-control and reusability.\nPhase 2: Building the Website Frontend and Hosting it on AWS Blog Post: Phase 2\nInitial design philosophy - using AWS S3 static website hosting and SSL certificates. Leveraging the use of CloudFront distribution for HTTPS access and global distribution. Creating the website code using a combination of HTML, CSS, YAML, JSON and HUGO. Phase 3: Building the Backend using AWS API Gateway, Lambda Functions, DynamoDB Blog Post: Phase 3\nBuilding the backend components starting with the database. Storing data on DynamoDB. Creating lambda function using python to update and return DynamoDB database information. Phase 4: Integrating the Frontend and Backend Blog Post: Phase 4\nIntegrating website frontend and backend services to create an example visitor counter stat. Creating and linking API Gateway to a lambda function.\nPhase 5: Deployment using IaC and using CI/CD workflows Blog Post: Phase 5\nGitHub Actions Workflow: leveraged GitHub Actions to automate CI/CD pipeline. This streamlined the process of deploying updates, ensuring that SAM application package and Python code were properly tested before deployment.\nFuture Plans Coming from a background in IT infrastructure and working on an on-prem environment for many years - completing this challenge has been eye-opening in a few ways. It\u0026rsquo;s important to adapt to new concepts for working with the cloud. Many design philosophies that I have learned can still be applied in a cloud environment.\nI would like to continue completing more projects to gain more exposure and hands-on experience to the resources and tools that AWS has to offer. I\u0026rsquo;ll be exploring other areas in the cloud.\n","permalink":"https://frankdoka.com/projects/frank-doka-project-1/","summary":"Intro The AWS Cloud Resume Challenge is an engaging, hands-on initiative designed to elevate AWS cloud skills. Participants are challenged to build a professional resume website hosted exclusively on AWS infrastructure. This practical project hones the ability to deploy, configure, and secure AWS services, including EC2, S3,Route 53, Lambda, DynamoDB and much more.\nIt offers an exceptional opportunity to showcase expertise to prospective employers while developing proficiency in cloud computing. Bolster AWS knowledge and create a tangible asset.","title":"Cloud Resume Challenge"},{"content":"Description Develop and lead large multifunctional system activities including managing, creating and deploying application programs and packages with various technologies such as Microsoft System Center Configuration Manager, Microsoft Deployment Toolkit, Docker.\nConduct application testing to validate codebase and remediate issues discovered through performance and security testing.\nInstall, configure, troubleshoot, document and proactively maintain Windows and Linux physical and virtual servers, systems and applications using VMWare and Hyper-V.\nMonitor, deploy and maintain server and desktop workstation patches and upgrades. Manage patching and maintenance activities through WSUS and SCCM. Design, build, implement and maintain Windows Active Directory and Group Policy Objects configuration. Troubleshoot application systems and provide end-user support and problem resolution.\nLed a team in designing and implemented custom OS imaging solutions as part of a large-scale migration of 10,000+ devices from Windows 7 to Windows 10 and Windows 11. ","permalink":"https://frankdoka.com/experience/frank-doka-experience-1/","summary":"Description Develop and lead large multifunctional system activities including managing, creating and deploying application programs and packages with various technologies such as Microsoft System Center Configuration Manager, Microsoft Deployment Toolkit, Docker.\nConduct application testing to validate codebase and remediate issues discovered through performance and security testing.\nInstall, configure, troubleshoot, document and proactively maintain Windows and Linux physical and virtual servers, systems and applications using VMWare and Hyper-V.\nMonitor, deploy and maintain server and desktop workstation patches and upgrades.","title":"NYC Department of Correction"},{"content":" You are visitor number loading...\n","permalink":"https://frankdoka.com/stats/visitor-count/","summary":"You are visitor number loading...","title":"Visitor Count"},{"content":"Description Provided IT technical support and remote assistance to end-users for computer operational and various peripheral equipment, resolving 1500+ support tickets\nAssigned and delegated various computer operational work to other technical units and ensured satisfactory completion of tasks.\nCollaborate with IT teams to ensure timely and effective resolution of technical issues and incidents.\n","permalink":"https://frankdoka.com/experience/frank-doka-experience-2/","summary":"Description Provided IT technical support and remote assistance to end-users for computer operational and various peripheral equipment, resolving 1500+ support tickets\nAssigned and delegated various computer operational work to other technical units and ensured satisfactory completion of tasks.\nCollaborate with IT teams to ensure timely and effective resolution of technical issues and incidents.","title":"NYC Police Department"}]