[{"content":"Smoke Testing, Infrastructure as Code (IAC), and CI/CD Enhancements We focused on enhancing our project with smoke testing, delved into Infrastructure as Code (IAC) using AWS Serverless Application Model (SAM) and Terraform, and continued to refine our Continuous Integration/Continuous Deployment (CI/CD) pipeline.\nSmoke Testing Install Cypress: We began the day by installing Cypress, a powerful end-to-end testing framework. Cypress would be instrumental in performing smoke tests to validate our system\u0026rsquo;s functionality.\nSmoke Test Execution: With Cypress in place, we conducted smoke tests to check the data being sent and to cover edge cases. These tests ensured that our system behaved as expected under various scenarios.\nInfrastructure as Code (IAC) AWS SAM Template: We explored AWS Serverless Application Model (SAM) templates, a convenient way to define our serverless infrastructure. This included configuring components like Lambda functions, DynamoDB, and API Gateway.\nCloudFormation and Terraform: We delved into two IAC tools, CloudFormation and Terraform, to manage our cloud resources. CloudFormation is AWS\u0026rsquo;s native tool for IAC, while Terraform is a versatile, multi-cloud tool widely adopted in the industry. Terraform uses a domain-specific configuration language called HCL.\nCI/CD Backend GitHub Actions Workflow: We leveraged GitHub Actions to automate our CI/CD pipeline. This streamlined the process of deploying updates, ensuring that our SAM application package and Python code were properly tested before deployment.\nWorkflow Steps:\nBuild Step: In this step, we packaged up our Lambda function code and installed any necessary dependencies. The output was a zipped Lambda function artifact and a configuration file ready for deploying resources. Deploy Step: We utilized our chosen IAC tool (Terraform) to deploy AWS resources based on the defined templates. Smoke Test Step: To ensure the API functioned as expected, we ran Cypress API tests, confirming that the deployed API met our requirements. Backend Infrastructure with Terraform Terraform Configuration: We began setting up our backend infrastructure as code using Terraform. This included creating a main.tf file and configuring an EC2 instance for testing purposes.\nLambda Function and DynamoDB Policies: We reviewed our Lambda function code and ensured that the necessary policies and permissions were correctly defined.\nDynamoDB Table Configuration: We created the base DynamoDB table and continued to explore how to add items to it programmatically.\nAPI Gateway Enhancements: We improved our API Gateway configuration by attaching the Lambda function and configuring the GET function. Additionally, we addressed CORS (Cross-Origin Resource Sharing) settings, allowing the GET method.\nGitHub Repository Setup Terraform and Backend Repository: We initiated a GitHub repository dedicated to our backend code and testing. This repository would host our Terraform configurations and serve as a central hub for backend development. As Day 5 concludes, we\u0026rsquo;ve made significant strides in our project, focusing on testing, infrastructure as code, and CI/CD enhancements. Our commitment to automation and robust testing ensures a reliable and scalable project. Stay tuned for further updates as we continue to fine-tune and expand our system.\n","permalink":"https://frankdoka.com/blog/resume-challenge-5/","summary":"Smoke Testing, Infrastructure as Code (IAC), and CI/CD Enhancements We focused on enhancing our project with smoke testing, delved into Infrastructure as Code (IAC) using AWS Serverless Application Model (SAM) and Terraform, and continued to refine our Continuous Integration/Continuous Deployment (CI/CD) pipeline.\nSmoke Testing Install Cypress: We began the day by installing Cypress, a powerful end-to-end testing framework. Cypress would be instrumental in performing smoke tests to validate our system\u0026rsquo;s functionality.","title":"IaC, CI/CD Enhancements, Smoke Testing"},{"content":"API Backend Continuation and Front-End Integration we continued our exploration of the API backend and initiated the crucial integration between the front end and back end. Let\u0026rsquo;s dive into the details of our progress:\nAPI Creation and Testing API Gateway Route for Lambda Function: We set up an API Gateway route in front of our Lambda function, configuring it to handle POST requests. This step was essential for creating a pathway for communication between our front end and back end.\nInternal Testing with API Gateway: We utilized the API Gateway\u0026rsquo;s internal test console to ensure that our API was functioning as expected. This involved making requests to our Lambda function and verifying that we received the correct responses.\nExternal Testing with Postman: To expand our testing scope, we tested our API using Postman. This allowed us to verify that our API worked seamlessly when accessed externally.\nIncrementing DynamoDB Count: We created an API Gateway for Lambda, which invoked the Lambda function to increment the count in our DynamoDB table. We tested this functionality through various means, including browser and Postman, to ensure that the count increased as expected.\nFront-End and Back-End Integration Connecting Front-End to Back-End: The next crucial step was integrating the front end (our resume site hosted on S3) with the back end (our API). We aimed to display the visitor count on the homepage of our site.\nAutomated Tests: We implemented automated tests to confirm the smooth operation of our integration. These tests were designed to validate that our system always performed as expected.\nJavaScript Visitor Counter: We created a JavaScript visitor counter that made HTTP requests to our API Gateway. While this was a straightforward task, we encountered some CORS (Cross-Origin Resource Sharing) issues between the browser and API, which we addressed. The goal was to display the visitor count on our web app.\nPython Code Testing: For comprehensive testing, we wrote automated tests against our API. These tests, often referred to as \u0026ldquo;smoke tests\u0026rdquo; or end-to-end tests, verified that our Lambda code worked correctly, permissions and configurations were accurate, and resources were deployed successfully. We employed Cypress, a JavaScript testing framework, to call our API endpoint and perform a series of checks:\nEnsuring the API returned an updated value. Verifying that the API correctly updated the database. Confirming that the API responded to unexpected input. Checking edge cases in our function logic, such as handling uninitialized visitor counts. Front-End Updates and CI/CD: We revisited the front end, incorporating it into our GitHub repository and enhancing our CI/CD (Continuous Integration/Continuous Deployment) pipeline. We also updated AWS secret keys for secure access.\nHugo Shortcode for Visitor Count: To make it easier to display the visitor count on our website, we created a Hugo shortcode and added a new Visitors tab to test shortcodes.\nFront-End Integration Challenges: While the front end successfully fetched data from DynamoDB and displayed the visitor count, we encountered issues with updating the count correctly. These issues were primarily related to CORS policy restrictions.\nCORS Policy Adjustment: To resolve the CORS issue, we made necessary changes to the CORS policy on AWS to allow the website and enable the GET method.\nWith these accomplishments and challenges tackled, we\u0026rsquo;re well on our way to creating a seamless integration between our front-end and back-end systems.\n","permalink":"https://frankdoka.com/blog/resume-challenge-4/","summary":"API Backend Continuation and Front-End Integration we continued our exploration of the API backend and initiated the crucial integration between the front end and back end. Let\u0026rsquo;s dive into the details of our progress:\nAPI Creation and Testing API Gateway Route for Lambda Function: We set up an API Gateway route in front of our Lambda function, configuring it to handle POST requests. This step was essential for creating a pathway for communication between our front end and back end.","title":"API Backend Continuation and Frontend Integration"},{"content":"Getting Started with Backend Technologies and APIs Today, we dived into the world of backend technologies and APIs, setting the foundation for our project\u0026rsquo;s functionality.\nJavaScript Code to API to Database (DynamoDB) Our mission for today was to establish the crucial link between our JavaScript code and the database. We opted for Amazon DynamoDB as our database solution and AWS services like API Gateway and Lambda to create a robust backend.\nSetting Up the Lambda Function Our first task was to get a Lambda function talking to the database. We set up a Python Lambda function using the AWS Web Console, laying the groundwork for processing requests from our front-end application.\nDynamoDB Table Setup Creating a DynamoDB table was a straightforward \u0026ldquo;point and click\u0026rdquo; process. This table would serve as our data store, enabling us to store and retrieve data efficiently.\nWriting Python Code for Lambda To make our Lambda function functional, we wrote Python code that would interact with DynamoDB. We tested this code from the AWS Console to ensure that data could be successfully saved to the database.\nIAM Permissions Configuration Ensuring secure access between our Lambda function and the DynamoDB table was paramount. We configured IAM permissions to manage access and control over who can interact with these AWS resources.\nExploring SAM (Serverless Application Model) We delved into the world of SAM, an open-source framework for building serverless applications. Our goal was to set up SAM locally, allowing us to develop and test Lambda functions and DynamoDB tables locally before deploying them to AWS.\nCreating the DynamoDB Database in AWS We created a DynamoDB database in our AWS environment and established the Lambda function\u0026rsquo;s connection to the database, with the appropriate IAM permissions to facilitate communication.\nAPI Development With our backend infrastructure in place, we started building the API. The API would enable our front-end website to communicate with the Lambda function, facilitating data transfer between the user interface and the database.\nAPI Gateway Configuration API Gateway was our choice for managing API endpoints. We configured it to route requests to our Lambda function, allowing for seamless data transfer between the website and the backend.\nTesting the API Locally We used the API Gateway\u0026rsquo;s internal test console to perform a \u0026ldquo;Hello World\u0026rdquo; test, ensuring that the API was correctly connected to our Lambda function.\nExternal Testing with Postman To ensure that our API was ready for prime time, we grabbed its public URL from the API Gateway and conducted tests using external API testing services like Postman. Our goal was to verify that the Lambda function produced the expected output.\nWith these steps, we\u0026rsquo;ve made significant progress in establishing our backend infrastructure and API. Stay tuned as we continue to develop and refine our project, bringing it one step closer to completion.\n","permalink":"https://frankdoka.com/blog/resume-challenge-3/","summary":"Getting Started with Backend Technologies and APIs Today, we dived into the world of backend technologies and APIs, setting the foundation for our project\u0026rsquo;s functionality.\nJavaScript Code to API to Database (DynamoDB) Our mission for today was to establish the crucial link between our JavaScript code and the database. We opted for Amazon DynamoDB as our database solution and AWS services like API Gateway and Lambda to create a robust backend.","title":"Backend Technologies and APIs"},{"content":"The second part of our cloud resume project journey! Today, we start building the front-end of our project, exploring new tools and techniques to enhance our skills and create a polished web presence.\nCrafting a Resume in HTML Our first task was to create a digital resume.\nWe decided to keep start simple and effective by coding it in HTML. This allowed us to structure the content precisely as we wanted it and provided a foundation for future styling.\nStyling with CSS To make our resume visually appealing, we utilized Cascading Style Sheets (CSS).\nWith CSS, we transformed our plain HTML content into an aesthetically pleasing document that showcased our skills and experience.\nHosting with a Static S3 Website, HTTPS, and DNS With our resume ready, it was time to make it accessible to the world.\nWe hosted it as a static website on Amazon S3, ensuring that it was served over HTTPS for security and that DNS was configured correctly for easy access.\nConfiguring Route 53 and Routing Policies To ensure seamless access to our website, we configured Amazon Route 53. This involved setting up routing policies and creating the necessary DNS records.\nAddressing Name Server Issues One challenge we faced was related to name servers. We had to ensure that our domain was correctly configured with the name servers of the hosted zone. This step was crucial for proper domain resolution.\nUtilizing CloudFront for Enhanced Website Hosting To boost the performance and security of our website, we utilized Amazon CloudFront.\nThis content delivery network (CDN) served as an intermediary between our basic S3 static hosting and the users, ensuring faster load times and added security.\nCreating SSL Certificate with ACM Manager Security is paramount, so we secured our website with an SSL certificate from AWS Certificate Manager (ACM).\nThis step provided encryption and trustworthiness to our site, making it safer for visitors.\nLeveraging Hugo for Website Design To give our website a professional and polished look, we turned to Hugo.\nThis static site generator helped us create a visually appealing and responsive website with ease.\nUsing GitHub and GitHub Actions for Deployment For efficient website deployment, we employed GitHub and GitHub Actions.\nThis streamlined the process and allowed us to automate deployments. However, we encountered a challenge related to images not showing up correctly.\nOvercoming Image Display Challenges To address the image display issues, we implemented solutions like PaperMod and adjusted the content and static paths on our Hugo HTML website. These changes ensured that images were displayed correctly, enhancing the overall user experience.\n","permalink":"https://frankdoka.com/blog/resume-challenge-2/","summary":"The second part of our cloud resume project journey! Today, we start building the front-end of our project, exploring new tools and techniques to enhance our skills and create a polished web presence.\nCrafting a Resume in HTML Our first task was to create a digital resume.\nWe decided to keep start simple and effective by coding it in HTML. This allowed us to structure the content precisely as we wanted it and provided a foundation for future styling.","title":"Building the Front-End"},{"content":"Setting Up the Management Account Establishment of a Management Account, which serves as the central hub for controlling our AWS resources. This account plays a pivotal role in orchestrating and managing our entire AWS infrastructure.\nBuilding an AWS Organization with org-formation tool To streamline the organization of our AWS accounts and resources, we leveraged the powerful org-formation tool. This nifty tool automates the process, making it easier to configure and manage our AWS Organization. With org-formation, we can define the structure of our organization, establish clear hierarchies, and maintain consistency across our AWS environment.\nThis tool takes the idea behind AWS Control Tower and enhances it with more features and capabilities.\nEmbracing CodeCommit and CodePipeline To ensure efficient version control and continuous integration/continuous deployment (CI/CD) processes, we embraced CodeCommit and CodePipeline.\nThese AWS services allow us to commit and push changes to our codebase seamlessly. With Git at our fingertips, we have a robust mechanism to manage code versions and automate the deployment pipeline.\nFor our front-end and back-end solutions later on in the project , we will be utilizing GitHub and GitHub Actions.\nImplementing Automated Billing Alerts To keep our expenses in check, we set up automated billing alerts. These alerts notify us when our AWS costs reach predefined thresholds, ensuring that we are always aware of our spending.\nIntegrating CodeCommit with Visual Studio Code For a seamless development experience, we integrated CodeCommit with Visual Studio Code (VS Code). This integration simplifies the code collaboration process and allows our developers to work efficiently within their preferred development environment.\nCreating a Development Account To maintain a clear separation between our production and development environments, we established a dedicated Development Account. This account serves as a safe sandbox for testing changes before they are pushed to the production environment. It\u0026rsquo;s a crucial step to ensure the stability and reliability of our systems.\nSetting Up AWS Single Sign-On (SSO) Enhancing the login experience for our team members was a priority. We implemented AWS Single Sign-On (SSO) to provide a more convenient and secure way for users to access AWS services. With SSO, users can access multiple AWS accounts and applications using a single set of credentials.\nLeveraging IAM Identity Center for Role Federation To further enhance our security posture and access management, we utilized IAM Identity Center for role federation. This ensures that users have the appropriate permissions to access AWS resources while maintaining a high level of security and control.\nWith these foundational steps in place, we\u0026rsquo;ve laid the groundwork for a robust and organized AWS environment.\n","permalink":"https://frankdoka.com/blog/resume-challenge-1/","summary":"Setting Up the Management Account Establishment of a Management Account, which serves as the central hub for controlling our AWS resources. This account plays a pivotal role in orchestrating and managing our entire AWS infrastructure.\nBuilding an AWS Organization with org-formation tool To streamline the organization of our AWS accounts and resources, we leveraged the powerful org-formation tool. This nifty tool automates the process, making it easier to configure and manage our AWS Organization.","title":"AWS Accounts and Organization"},{"content":"Credentials 🔗 Credly Badge Introduction I passed SAA-C03 - AWS Solutions Architect - Associate certification exam.\nCertification Information As Cloud Computing continues to ascend, businesses are in a constant state of transition, moving away from traditional on-premise infrastructure towards the cloud. This migration brings a wealth of advantages, primarily in terms of scalability and resilience when confronted with unexpected disasters.\nThe AWS Solutions Architect - Associate certification stands as a testament to your proficiency in crafting and deploying meticulously planned solutions within the AWS ecosystem, currently the foremost cloud provider in the industry. In essence, this examination gauges your aptitude for conceiving architectural designs tailored to specific scenarios. To illustrate, consider a scenario where a company desires the uninterrupted operation of its application, even in the face of a catastrophic event causing an entire AWS region to become inoperable. In this context, how would you strategically structure their infrastructure to meet this exigent requirement?\nExam Format In this examination, you will encounter a total of 65 questions and have a time allotment of 130 minutes to respond to them. This equates to an average of 2 minutes per question for you to carefully consider and answer. Your evaluation will be established on a percentile scale that spans from 100 to 1000, with a requisite score of over 720 to attain a passing grade. With this information in mind, you can deduce that achieving approximately 72% accuracy in your responses is essential for success. For a more comprehensive understanding of the scoring system, you can consult this informative link.\nThis examination operates under a binary pass/fail criterion. Should you successfully pass the exam, the specific score achieved will have no substantial bearing. Your score will merely be included in your score report for personal reference, without appearing on the official certificate.\nThe examination fee for this assessment is set at $150 USD.\nPrep Strategy for SAA-C03 Exam Exam Blueprint Understanding: Begin by thoroughly reviewing the official AWS Certified Solutions Architect - Associate exam guide to understand key domains.\nStudy Plan: Create a flexible study schedule allocating time for courses, practice exams, hands-on labs, and revision.\nOnline Resources: Enroll in reputable online courses tailored for the SAA-C03 exam. Popular platforms like AWS Training and A Cloud Guru offer comprehensive video courses.\nHands-On Practice: Use a free AWS Free Tier account to gain practical experience deploying AWS services and solutions.\nPractice Exams: Take official AWS practice exams, available on the AWS Training website.\nCommunity Engagement: Join AWS forums, such as the AWS Developer Forums.\nRegular Review: Periodically revisit study materials to reinforce your understanding, focusing on weaker areas.\nExam Simulation: Simulate the exam environment with timed practice exams as your exam date approaches.\nIdentify Weaknesses: Analyze practice exam results to pinpoint areas needing improvement.\nExam Logistics: Familiarize yourself with exam details and arrive early on the exam day with proper identification.\nPost-Exam Analysis: Regardless of the outcome, review areas of difficulty for future improvement.\nContinuous Learning: Stay updated with AWS announcements through the AWS Blog, AWS What\u0026rsquo;s New, and consider advanced AWS certifications after achieving SAA-C03.\n","permalink":"https://frankdoka.com/blog/aws-saa-certification/","summary":"Credentials 🔗 Credly Badge Introduction I passed SAA-C03 - AWS Solutions Architect - Associate certification exam.\nCertification Information As Cloud Computing continues to ascend, businesses are in a constant state of transition, moving away from traditional on-premise infrastructure towards the cloud. This migration brings a wealth of advantages, primarily in terms of scalability and resilience when confronted with unexpected disasters.\nThe AWS Solutions Architect - Associate certification stands as a testament to your proficiency in crafting and deploying meticulously planned solutions within the AWS ecosystem, currently the foremost cloud provider in the industry.","title":"AWS SAA Certification Exam"},{"content":"My name is Frank Doka. I am currently located in New York City.\nThese days, I’m working with different cloud technologies and integrations such as AWS, containers, CI/CD and using infra-as-code tools such as Terraform.\nOne of my passions is Systems Engineering. Finding the best way to improve reliability and scalability of various technologies. And streamlining the process of pushing code into production.\nI have a keen interest in the field of Automation and applying it into the work I produce. Creating high quality and re-usuable code through scripts written in various languages is something that I strive to achieve.\nI have over 6 years of system engineering experience in the public sector working for the City of New York as a Systems Administrator. I am looking to advance my career and skillset into the Cloud and incorporate my existing skills and knowledge to provide positive growth to the companies I work with as well as my own personal growth.\n","permalink":"https://frankdoka.com/about/","summary":"My name is Frank Doka. I am currently located in New York City.\nThese days, I’m working with different cloud technologies and integrations such as AWS, containers, CI/CD and using infra-as-code tools such as Terraform.\nOne of my passions is Systems Engineering. Finding the best way to improve reliability and scalability of various technologies. And streamlining the process of pushing code into production.\nI have a keen interest in the field of Automation and applying it into the work I produce.","title":"About Me"},{"content":"Intro The AWS Cloud Resume Challenge is an engaging, hands-on initiative designed to elevate your AWS cloud skills. Participants are challenged to build a professional resume website hosted exclusively on AWS infrastructure. This practical project hones your ability to deploy, configure, and secure AWS services, including EC2, S3,Route 53, Lambda, DynamoDB and much more.\nIt offers an exceptional opportunity to showcase your expertise to prospective employers while developing proficiency in cloud computing. By completing this challenge, you not only bolster your AWS knowledge but also create a tangible asset for your career, making you a more competitive candidate in the ever-evolving field of cloud technology.\nProject Overview This is a high level overview of the steps I took during this challenge:\nHTML Resume: Start by creating a professional resume using HTML.\nCSS Styling: Style your resume using CSS to enhance its visual appeal and formatting.\nStatic Website: Host your resume on a static S3 website. Optionally, consider utilizing an EC2 instance if required.\nHTTPS Security: Ensure the security of your S3 website by enabling HTTPS. Utilize Amazon CloudFront for this purpose.\nCustom DNS: Point a custom DNS name to your CloudFront distribution using Amazon Route 53 for a personalized web address.\nJavaScript Visitor Counter: Implement a visitor counter on your website using JavaScript to display the number of site visitors.\nDatabase for Visitor Counter: Use DynamoDB to store and manage the visitor counter data efficiently, benefiting from its on-demand pricing model.\nAPI Creation: Develop an API that communicates with the database and accepts requests from your web app. AWS API Gateway combined with Lambda can help accomplish this.\nPython Integration: Utilize Python for Lambda functions to enhance the functionality of your serverless architecture.\nTesting: Implement robust testing for your Python code to ensure its reliability and performance.\nInfrastructure as Code (IaC): Configure AWS resources like DynamoDB tables, API Gateway, and Lambda functions using an AWS SAM (Serverless Application Model) template. Deploy these resources effortlessly using AWS SAM CLI.\nSource Control: Employ GitHub for version control, enabling seamless updates to both the back-end API and front-end website.\nCI/CD (Backend): Implement GitHub Actions to automate tests and deployments. Whenever updates are pushed to your SAM template or Python code, tests are executed, and if successful, the SAM app is packaged and deployed to AWS.\nCI/CD (Frontend): Maintain a separate GitHub repository for your website code. Configure GitHub Actions to automatically update the S3 bucket when code changes occur (consider adding logic to invalidate CloudFront cache in code). Avoid committing AWS credentials to your code.\nBlog Post Integration: Include a link to a concise blog post within your resume text. This post should highlight key learnings and experiences gained throughout the project.\n","permalink":"https://frankdoka.com/projects/frank-doka-project-1/","summary":"Intro The AWS Cloud Resume Challenge is an engaging, hands-on initiative designed to elevate your AWS cloud skills. Participants are challenged to build a professional resume website hosted exclusively on AWS infrastructure. This practical project hones your ability to deploy, configure, and secure AWS services, including EC2, S3,Route 53, Lambda, DynamoDB and much more.\nIt offers an exceptional opportunity to showcase your expertise to prospective employers while developing proficiency in cloud computing.","title":"Cloud Resume Challenge"},{"content":"Description Develop and lead large multifunctional system activities including managing, creating and deploying application programs and packages with various technologies such as Microsoft System Center Configuration Manager, Microsoft Deployment Toolkit, Docker.\nConduct application testing to validate codebase and remediate issues discovered through performance and security testing.\nInstall, configure, troubleshoot, document and proactively maintain Windows and Linux physical and virtual servers, systems and applications using VMWare and Hyper-V.\nMonitor, deploy and maintain server and desktop workstation patches and upgrades. Manage patching and maintenance activities through WSUS and SCCM. Design, build, implement and maintain Windows Active Directory and Group Policy Objects configuration. Troubleshoot application systems and provide end-user support and problem resolution.\nLed a team in designing and implemented custom OS imaging solutions as part of a large-scale migration of 10,000+ devices from Windows 7 to Windows 10 and Windows 11. ","permalink":"https://frankdoka.com/experience/frank-doka-experience-1/","summary":"Description Develop and lead large multifunctional system activities including managing, creating and deploying application programs and packages with various technologies such as Microsoft System Center Configuration Manager, Microsoft Deployment Toolkit, Docker.\nConduct application testing to validate codebase and remediate issues discovered through performance and security testing.\nInstall, configure, troubleshoot, document and proactively maintain Windows and Linux physical and virtual servers, systems and applications using VMWare and Hyper-V.\nMonitor, deploy and maintain server and desktop workstation patches and upgrades.","title":"NYC Department of Correction"},{"content":" You are visitor number loading...\n","permalink":"https://frankdoka.com/stats/visitor-count/","summary":"You are visitor number loading...","title":"Visitor Count"},{"content":"Description Provided IT technical support and remote assistance to end-users for computer operational and various peripheral equipment, resolving 1500+ support tickets\nAssigned and delegated various computer operational work to other technical units and ensured satisfactory completion of tasks.\nCollaborate with IT teams to ensure timely and effective resolution of technical issues and incidents.\n","permalink":"https://frankdoka.com/experience/frank-doka-experience-2/","summary":"Description Provided IT technical support and remote assistance to end-users for computer operational and various peripheral equipment, resolving 1500+ support tickets\nAssigned and delegated various computer operational work to other technical units and ensured satisfactory completion of tasks.\nCollaborate with IT teams to ensure timely and effective resolution of technical issues and incidents.","title":"NYC Police Department"}]